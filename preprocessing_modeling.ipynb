{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:58:34.594372Z",
     "start_time": "2019-11-11T08:58:29.515108Z"
    }
   },
   "outputs": [],
   "source": [
    "#預處理\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#可視化\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#ML\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lightgbm as lgbm\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (GradientBoostingClassifier, GradientBoostingRegressor, \n",
    "                              RandomForestClassifier, RandomForestRegressor)\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost import datasets\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:58:37.498357Z",
     "start_time": "2019-11-11T08:58:34.595327Z"
    }
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('submission_test.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:58:37.852000Z",
     "start_time": "2019-11-11T08:58:37.500351Z"
    }
   },
   "outputs": [],
   "source": [
    "data = train.append(test)\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing- Label encoding for Randomforest and Lightgbm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:58:38.785398Z",
     "start_time": "2019-11-11T08:58:37.853504Z"
    }
   },
   "outputs": [],
   "source": [
    "#for both RF and Lgbm\n",
    "data['ecfg'] = data['ecfg'].astype('category').cat.codes\n",
    "data['flbmk'] = data['flbmk'].astype('category').cat.codes\n",
    "data['flg_3dsmk'] = data['flg_3dsmk'].astype('category').cat.codes\n",
    "data['insfg'] = data['insfg'].astype('category').cat.codes\n",
    "data['ovrlt'] = data['ovrlt'].astype('category').cat.codes\n",
    "#only for lgbm\n",
    "data['acqic'] = data['acqic'].astype('category').cat.codes\n",
    "data['bacno'] = data['bacno'].astype('category').cat.codes\n",
    "data['cano'] = data['cano'].astype('category').cat.codes\n",
    "data['csmcu'] = data['csmcu'].astype('category').cat.codes\n",
    "data['hcefg'] = data['hcefg'].astype('category').cat.codes\n",
    "data['mcc'] = data['mcc'].astype('category').cat.codes\n",
    "data['mchno'] = data['mchno'].astype('category').cat.codes\n",
    "data['scity'] = data['scity'].astype('category').cat.codes\n",
    "data['stocn'] = data['stocn'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:58:38.790385Z",
     "start_time": "2019-11-11T08:58:38.786396Z"
    }
   },
   "outputs": [],
   "source": [
    "#Handling Missing Values\n",
    "# data['flg_3dsmk'] = data[['ecfg','flg_3dsmk']].apply(lambda x: x['ecfg'] if x['ecfg']=='N' else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['stocn','flg_3dsmk']].apply(lambda x: 0 if x['stocn']==46 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['stocn','flg_3dsmk']].apply(lambda x: 0 if x['stocn']==49 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['stocn','flg_3dsmk']].apply(lambda x: 0 if x['stocn']==47 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==5 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==31 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==53 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==205 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==221 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==226 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==356 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==361 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==372 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==407 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==421 else x['flg_3dsmk'], axis=1)\n",
    "\n",
    "# data['flbmk'] = data[['ecfg','flbmk']].apply(lambda x:  'N'if x['ecfg']=='Y' else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['contp','flbmk']].apply(lambda x: 'N' if x['contp']==2 else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['contp','flbmk']].apply(lambda x: 'N' if x['contp']==4 else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['contp','flbmk']].apply(lambda x: 'N' if x['contp']==6 else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['etymd','flbmk']].apply(lambda x: 'N' if x['etymd']==0 else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['etymd','flbmk']].apply(lambda x: 'N' if x['etymd']==1 else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['etymd','flbmk']].apply(lambda x: 'N' if x['etymd']==2 else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['etymd','flbmk']].apply(lambda x: 'N' if x['etymd']==4 else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['etymd','flbmk']].apply(lambda x: 'N' if x['etymd']==5 else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['etymd','flbmk']].apply(lambda x: 'N' if x['etymd']==6 else x['flbmk'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:58:41.293392Z",
     "start_time": "2019-11-11T08:58:41.215601Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    if data[col].dtype=='float64': data[col] = data[col].astype('float32')\n",
    "    if data[col].dtype=='int64': data[col] = data[col].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['loctm_int'] = data['loctm'].astype(int)\n",
    "data['loctm_str'] = data['loctm_int'].astype(str)\n",
    "data['loctm_str'] = data['loctm_str'].apply(lambda x : '0'+x if (len(x)==5) else ('00'+x if (len(x)==4) else ('000'+x if (len(x)==3) else ('0000'+x if (len(x)==2) else ('00000'+x if (len(x)==1) else x)))))\n",
    "data['hours'] = data['loctm_str'].str[0:2]\n",
    "data['minutes'] = data['loctm_str'].str[2:4]\n",
    "data['seconds'] = data['loctm_str'].str[4:6]\n",
    "data['hours'] = data['hours'].astype(int)\n",
    "data['minutes'] = data['minutes'].astype(int)\n",
    "data['seconds'] = data['seconds'].astype(int)\n",
    "data['total_seconds'] = data['locdt']*86400+data['hours']*3600+data['minutes']*60+data['seconds']\n",
    "data['time'] = (data['hours']*3600+data['minutes']*60+data['seconds'])#/86400\n",
    "data['timeBin'] = pd.cut(data['time'], 24)\n",
    "data['timeBin'] = data['timeBin'].astype('category').cat.codes\n",
    "data['timeBin4'] = pd.cut(data['time'], 4)\n",
    "data['timeBin4'] = data['timeBin'].astype('category').cat.codes\n",
    "data['timeBin5'] = pd.cut(data['time'], 5)\n",
    "data['timeBin5'] = data['timeBin'].astype('category').cat.codes\n",
    "data['timeBin6'] = pd.cut(data['time'], 6)\n",
    "data['timeBin6'] = data['timeBin'].astype('category').cat.codes\n",
    "\n",
    "data['decimal'] = data['conam'] - data['conam'].astype(int)\n",
    "data['taiwan_ornot'] = data['stocn'].apply(lambda x : 1 if x==102 else 0)\n",
    "\n",
    "data['dow'] = data['locdt'] % 7\n",
    "data['date_day'] = data['locdt'] % 30\n",
    "data['percentage'] = data['locdt'] / 90\n",
    "data['2_week'] = data['locdt'] % 14\n",
    "data['month'] = data['locdt'] % 30\n",
    "\n",
    "data['stocn_csmcu'] = data['stocn'].astype('str') + data['csmcu'].astype('str')\n",
    "data['ecfg_stocn'] = data['ecfg'] + data['stocn'].astype('str')\n",
    "data['ecfg_scity'] = data['ecfg'] + data['scity'].astype('str')\n",
    "data['ovrlt_stocn'] = data['ovrlt'] + data['stocn'].astype('str')\n",
    "data['ovrlt_scity'] = data['ovrlt'] + data['stocn'].astype('str')\n",
    "data['stscd_mchno'] = data['stscd'].astype('str') + data['mchno'].astype('str')\n",
    "data['stscd_mcc'] = data['stscd'].astype('str') + data['mcc'].astype('str')\n",
    "data['mchno_mcc'] = data['mchno'].astype('str') + data['mcc'].astype('str')\n",
    "data['week_timeBin4'] = data['week'].astype('str') + data['timeBin4'].astype('str')\n",
    "data['week_timeBin5'] = data['week'].astype('str') + data['timeBin5'].astype('str')\n",
    "data['week_timeBin6'] = data['week'].astype('str') + data['timeBin6'].astype('str')\n",
    "data['week_hours'] = data['week'].astype('str') + data['hours'].astype('str')\n",
    "data['stocn_scity'] = data['stocn'].astype('str') + data['scity'].astype('str')\n",
    "data['flbmk_cat'] = data['flbmk'].astype('category').cat.codes\n",
    "data['ovrlt_flbmk_cat'] = data['ovrlt'] + data['flbmk_cat'].astype('str')\n",
    "data['flg_3dsmk_cat'] = data['flg_3dsmk'].astype('category').cat.codes\n",
    "data['flg_3dsmk_cat_flbmk_cat'] = data['flg_3dsmk_cat'].astype('str') + data['flbmk_cat'].astype('str')\n",
    "data['ovrlt_hcefg'] = data['ovrlt'] + data['hcefg'].astype('str')\n",
    "data['contp_etymd'] = data['contp'].astype('str') + data['etymd'].astype('str')\n",
    "data['ecfg_stscd'] = data['ecfg'] + data['stscd'].astype('str')\n",
    "data['bacno_cano'] = data['bacno'].astype('str') + data['cano'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate\n",
    "train = data[pd.notnull(data['fraud_ind'])]\n",
    "test = data[~pd.notnull(data['fraud_ind'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transation_count = train.groupby(['bacno'])['cano'].count().rename(\"transation_count\").reset_index()\n",
    "train = train.merge(transation_count)\n",
    "transation_count = test.groupby(['bacno'])['cano'].count().rename(\"transation_count\").reset_index()\n",
    "test = test.merge(transation_count)\n",
    "\n",
    "transation_count_cano = train.groupby(['bacno','cano'])['cano'].count().rename(\"transation_count_cano\").reset_index()\n",
    "train = train.merge(transation_count_cano)\n",
    "transation_count_cano = test.groupby(['bacno','cano'])['cano'].count().rename(\"transation_count_cano\").reset_index()\n",
    "test = test.merge(transation_count_cano)\n",
    "\n",
    "acqic_duplicated_count = train.groupby(['bacno','acqic'])['acqic'].count().rename(\"acqic_duplicated_count\").reset_index()\n",
    "train = train.merge(acqic_duplicated_count)\n",
    "acqic_duplicated_count = test.groupby(['bacno','acqic'])['acqic'].count().rename(\"acqic_duplicated_count\").reset_index()\n",
    "test = test.merge(acqic_duplicated_count)\n",
    "\n",
    "conam_duplicated_count = train.groupby(['bacno','conam'])['conam'].count().rename(\"conam_duplicated_count\").reset_index()\n",
    "train = train.merge(conam_duplicated_count)\n",
    "conam_duplicated_count = test.groupby(['bacno','conam'])['conam'].count().rename(\"conam_duplicated_count\").reset_index()\n",
    "test = test.merge(conam_duplicated_count)\n",
    "\n",
    "conam_stocn = train.groupby(['csmcu','stocn'])['stocn'].count().rename(\"conam_stocn\").reset_index()\n",
    "train = train.merge(conam_stocn, on=['csmcu', 'stocn'])\n",
    "conam_stocn = test.groupby(['csmcu','stocn'])['stocn'].count().rename(\"conam_stocn\").reset_index()\n",
    "test = test.merge(conam_stocn, on=['csmcu', 'stocn'])\n",
    "\n",
    "bacno_mchno = train.groupby(['bacno','mchno'])['mchno'].count().rename(\"bacno_mchno\").reset_index()\n",
    "train = train.merge(bacno_mchno, on=['bacno', 'mchno'])\n",
    "bacno_mchno = test.groupby(['bacno','mchno'])['mchno'].count().rename(\"bacno_mchno\").reset_index()\n",
    "test = test.merge(bacno_mchno, on=['bacno', 'mchno'])\n",
    "\n",
    "cano_mchno = train.groupby(['cano','mchno'])['mchno'].count().rename(\"cano_mchno\").reset_index()\n",
    "train = train.merge(cano_mchno, on=['cano', 'mchno'])\n",
    "cano_mchno = test.groupby(['cano','mchno'])['mchno'].count().rename(\"cano_mchno\").reset_index()\n",
    "test = test.merge(cano_mchno, on=['cano', 'mchno'])\n",
    "\n",
    "bacno_stocn = train.groupby(['bacno','stocn'])['stocn'].count().rename(\"bacno_stocn\").reset_index()\n",
    "train = train.merge(bacno_stocn, on=['bacno', 'stocn'])\n",
    "bacno_stocn = test.groupby(['bacno','stocn'])['stocn'].count().rename(\"bacno_stocn\").reset_index()\n",
    "test = test.merge(bacno_stocn, on=['bacno', 'stocn'])\n",
    "\n",
    "cano_stocn = train.groupby(['cano','stocn'])['stocn'].count().rename(\"cano_stocn\").reset_index()\n",
    "train = train.merge(cano_stocn, on=['cano', 'stocn'])\n",
    "cano_stocn = test.groupby(['cano','stocn'])['stocn'].count().rename(\"cano_stocn\").reset_index()\n",
    "test = test.merge(cano_stocn, on=['cano', 'stocn'])\n",
    "\n",
    "bacno_scity = train.groupby(['bacno','scity'])['scity'].count().rename(\"bacno_scity\").reset_index()\n",
    "train = train.merge(bacno_scity, on=['bacno', 'scity'])\n",
    "bacno_scity = test.groupby(['bacno','scity'])['scity'].count().rename(\"bacno_scity\").reset_index()\n",
    "test = test.merge(bacno_scity, on=['bacno', 'scity'])\n",
    "\n",
    "cano_scity = train.groupby(['cano','scity'])['scity'].count().rename(\"cano_scity\").reset_index()\n",
    "train = train.merge(cano_scity, on=['cano', 'scity'])\n",
    "cano_scity = test.groupby(['cano','scity'])['scity'].count().rename(\"cano_scity\").reset_index()\n",
    "test = test.merge(cano_scity, on=['cano', 'scity'])\n",
    "\n",
    "bacno_ecfg = train.groupby(['bacno','ecfg'])['ecfg'].count().rename(\"bacno_ecfg\").reset_index()\n",
    "train = train.merge(bacno_ecfg, on=['bacno', 'ecfg'])\n",
    "bacno_ecfg = test.groupby(['bacno','ecfg'])['ecfg'].count().rename(\"bacno_ecfg\").reset_index()\n",
    "test = test.merge(bacno_ecfg, on=['bacno', 'ecfg'])\n",
    "\n",
    "cano_ecfg = train.groupby(['cano','ecfg'])['ecfg'].count().rename(\"cano_ecfg\").reset_index()\n",
    "train = train.merge(cano_ecfg, on=['cano', 'ecfg'])\n",
    "cano_ecfg = test.groupby(['cano','ecfg'])['ecfg'].count().rename(\"cano_ecfg\").reset_index()\n",
    "test = test.merge(cano_ecfg, on=['cano', 'ecfg'])\n",
    "\n",
    "bacno_flg_3dsmk = train.groupby(['bacno','flg_3dsmk'])['flg_3dsmk'].count().rename(\"bacno_flg_3dsmk\").reset_index()\n",
    "train = train.merge(bacno_flg_3dsmk, on=['bacno', 'flg_3dsmk'])\n",
    "bacno_flg_3dsmk = test.groupby(['bacno','flg_3dsmk'])['flg_3dsmk'].count().rename(\"bacno_flg_3dsmk\").reset_index()\n",
    "test = test.merge(bacno_flg_3dsmk, on=['bacno', 'flg_3dsmk'])\n",
    "\n",
    "cano_flg_3dsmk = train.groupby(['cano','flg_3dsmk'])['flg_3dsmk'].count().rename(\"cano_flg_3dsmk\").reset_index()\n",
    "train = train.merge(cano_flg_3dsmk, on=['cano', 'flg_3dsmk'])\n",
    "cano_flg_3dsmk = test.groupby(['cano','flg_3dsmk'])['flg_3dsmk'].count().rename(\"cano_flg_3dsmk\").reset_index()\n",
    "test = test.merge(cano_flg_3dsmk, on=['cano', 'flg_3dsmk'])\n",
    "\n",
    "train['ecfg_cat'] = train['ecfg'].astype('category').cat.codes\n",
    "test['ecfg_cat'] = test['ecfg'].astype('category').cat.codes\n",
    "bacno_ecfg_cat_mean = train.groupby(['bacno'])['ecfg_cat'].mean().rename(\"bacno_ecfg_cat_mean\").reset_index()\n",
    "train = train.merge(bacno_ecfg_cat_mean, on=['bacno'])\n",
    "bacno_ecfg_cat_mean = test.groupby(['bacno'])['ecfg_cat'].mean().rename(\"bacno_ecfg_cat_mean\").reset_index()\n",
    "test = test.merge(bacno_ecfg_cat_mean, on=['bacno'])\n",
    "\n",
    "cano_ecfg_cat_mean = train.groupby(['cano'])['ecfg_cat'].mean().rename(\"cano_ecfg_cat_mean\").reset_index()\n",
    "train = train.merge(cano_ecfg_cat_mean, on=['cano'])\n",
    "cano_ecfg_cat_mean = test.groupby(['cano'])['ecfg_cat'].mean().rename(\"cano_ecfg_cat_mean\").reset_index()\n",
    "test = test.merge(cano_ecfg_cat_mean, on=['cano'])\n",
    "\n",
    "train = train.sort_values(by = ['total_seconds'])\n",
    "test = test.sort_values(by = ['total_seconds'])\n",
    "train['bacno_cumcount'] = train.groupby(['bacno']).cumcount()\n",
    "test['bacno_cumcount'] = test.groupby(['bacno']).cumcount()\n",
    "train['cano_cumcount'] = train.groupby(['cano']).cumcount()\n",
    "test['cano_cumcount'] = test.groupby(['cano']).cumcount()\n",
    "\n",
    "bacno_cumcount_max = train.groupby(['bacno'])['bacno_cumcount'].max().rename(\"bacno_cumcount_max\").reset_index()\n",
    "train = train.merge(bacno_cumcount_max)\n",
    "bacno_cumcount_max = test.groupby(['bacno'])['bacno_cumcount'].max().rename(\"bacno_cumcount_max\").reset_index()\n",
    "test = test.merge(bacno_cumcount_max)\n",
    "cano_cumcount_max = train.groupby(['cano'])['cano_cumcount'].max().rename(\"cano_cumcount_max\").reset_index()\n",
    "train = train.merge(cano_cumcount_max)\n",
    "cano_cumcount_max = test.groupby(['cano'])['cano_cumcount'].max().rename(\"cano_cumcount_max\").reset_index()\n",
    "test = test.merge(cano_cumcount_max)\n",
    "\n",
    "train['bacno_cumcount_percentage'] = train['bacno_cumcount']/train['bacno_cumcount_max']\n",
    "test['bacno_cumcount_percentage'] = test['bacno_cumcount']/test['bacno_cumcount_max']\n",
    "train['cano_cumcount_percentage'] = train['cano_cumcount']/train['cano_cumcount_max']\n",
    "test['cano_cumcount_percentage'] = test['cano_cumcount']/test['cano_cumcount_max']\n",
    "\n",
    "train['cano_sequence'] = train['cano_cumcount_percentage'].apply(lambda x : x if (x==0) else (x if (x==1) else 2))\n",
    "test['cano_sequence'] = test['cano_cumcount_percentage'].apply(lambda x : x if (x==0) else (x if (x==1) else 2))\n",
    "train['bacno_sequence'] = train['bacno_cumcount_percentage'].apply(lambda x : x if (x==0) else (x if (x==1) else 2))\n",
    "test['bacno_sequence'] = test['bacno_cumcount_percentage'].apply(lambda x : x if (x==0) else (x if (x==1) else 2))\n",
    "\n",
    "bacno_flg_3dsmk_mean = train.groupby(['bacno'])['flg_3dsmk'].mean().rename(\"bacno_flg_3dsmk_mean\").reset_index()\n",
    "train = train.merge(bacno_flg_3dsmk_mean, on=['bacno'])\n",
    "bacno_flg_3dsmk_mean = test.groupby(['bacno'])['flg_3dsmk'].mean().rename(\"bacno_flg_3dsmk_mean\").reset_index()\n",
    "test = test.merge(bacno_flg_3dsmk_mean, on=['bacno'])\n",
    "\n",
    "cano_flg_3dsmk_mean = train.groupby(['cano'])['flg_3dsmk'].mean().rename(\"cano_flg_3dsmk_mean\").reset_index()\n",
    "train = train.merge(cano_flg_3dsmk_mean, on=['cano'])\n",
    "cano_flg_3dsmk_mean = test.groupby(['cano'])['flg_3dsmk'].mean().rename(\"cano_flg_3dsmk_mean\").reset_index()\n",
    "test = test.merge(cano_flg_3dsmk_mean, on=['cano'])\n",
    "\n",
    "mean_amount = train.groupby(['bacno'])['conam'].mean().rename(\"mean_amount\").reset_index()\n",
    "train = train.merge(mean_amount)\n",
    "mean_amount = test.groupby(['bacno'])['conam'].mean().rename(\"mean_amount\").reset_index()\n",
    "test = test.merge(mean_amount)\n",
    "\n",
    "train['amtby_mean_amount'] = train['conam'] / train['mean_amount']\n",
    "test['amtby_mean_amount'] = test['conam'] / test['mean_amount']\n",
    "\n",
    "train['conam_mean_diff'] = train['conam'] - train['mean_amount']\n",
    "test['conam_mean_diff'] = test['conam'] - train['mean_amount']\n",
    "\n",
    "mean_amount_cano = train.groupby(['bacno','cano'])['conam'].mean().rename(\"mean_amount_cano\").reset_index()\n",
    "train = train.merge(mean_amount_cano)\n",
    "mean_amount_cano = test.groupby(['bacno','cano'])['conam'].mean().rename(\"mean_amount_cano\").reset_index()\n",
    "test = test.merge(mean_amount_cano)\n",
    "\n",
    "train['amtby_mean_amount_cano'] = train['conam'] / train['mean_amount_cano']\n",
    "test['amtby_mean_amount_cano'] = test['conam'] / test['mean_amount_cano']\n",
    "\n",
    "median_amount = train.groupby(['bacno'])['conam'].median().rename(\"median_amount\").reset_index()\n",
    "train = train.merge(median_amount)\n",
    "median_amount = test.groupby(['bacno'])['conam'].median().rename(\"median_amount\").reset_index()\n",
    "test = test.merge(median_amount)\n",
    "\n",
    "train['amtby_median_amount'] = train['conam'] / train['median_amount']\n",
    "test['amtby_median_amount'] = test['conam'] / test['median_amount']\n",
    "\n",
    "median_amount_cano = train.groupby(['bacno','cano'])['conam'].median().rename(\"median_amount_cano\").reset_index()\n",
    "train = train.merge(median_amount_cano)\n",
    "median_amount_cano = test.groupby(['bacno','cano'])['conam'].median().rename(\"median_amount_cano\").reset_index()\n",
    "test = test.merge(median_amount_cano)\n",
    "\n",
    "train['amtby_median_amount_cano'] = train['conam'] / train['median_amount_cano']\n",
    "test['amtby_median_amount_cano'] = test['conam'] / test['median_amount_cano']\n",
    "\n",
    "std_amount = train.groupby(['bacno'])['conam'].std().rename(\"std_amount\").reset_index()\n",
    "train = train.merge(std_amount)\n",
    "std_amount = test.groupby(['bacno'])['conam'].std().rename(\"std_amount\").reset_index()\n",
    "test = test.merge(std_amount)\n",
    "\n",
    "std_amount_cano = train.groupby(['bacno','cano'])['conam'].std().rename(\"std_amount_cano\").reset_index()\n",
    "train = train.merge(std_amount_cano)\n",
    "std_amount_cano = test.groupby(['bacno','cano'])['conam'].std().rename(\"std_amount_cano\").reset_index()\n",
    "test = test.merge(std_amount_cano)\n",
    "\n",
    "frequency = ((train.groupby(['bacno'])['locdt'].max()-train.groupby(['bacno'])['locdt'].min())/train.groupby(['bacno'])['locdt'].count()).rename(\"frequency\").reset_index()\n",
    "train = train.merge(frequency)\n",
    "frequency = ((test.groupby(['bacno'])['locdt'].max()-test.groupby(['bacno'])['locdt'].min())/test.groupby(['bacno'])['locdt'].count()).rename(\"frequency\").reset_index()\n",
    "test = test.merge(frequency)\n",
    "\n",
    "frequency_cano = ((train.groupby(['bacno','cano'])['locdt'].max()-train.groupby(['bacno','cano'])['locdt'].min())/train.groupby(['bacno','cano'])['locdt'].count()).rename(\"frequency_cano\").reset_index()\n",
    "train = train.merge(frequency_cano)\n",
    "frequency_cano = ((test.groupby(['bacno','cano'])['locdt'].max()-test.groupby(['bacno','cano'])['locdt'].min())/test.groupby(['bacno','cano'])['locdt'].count()).rename(\"frequency_cano\").reset_index()\n",
    "test = test.merge(frequency_cano)\n",
    "\n",
    "mean_time = train.groupby(['bacno'])['time'].mean().rename(\"mean_time\").reset_index()\n",
    "train = train.merge(mean_time)\n",
    "mean_time = test.groupby(['bacno'])['time'].mean().rename(\"mean_time\").reset_index()\n",
    "test = test.merge(mean_time)\n",
    "\n",
    "median_time = train.groupby(['bacno'])['time'].median().rename(\"median_time\").reset_index()\n",
    "train = train.merge(median_time)\n",
    "median_time = test.groupby(['bacno'])['time'].median().rename(\"median_time\").reset_index()\n",
    "test = test.merge(median_time)\n",
    "\n",
    "mean_time_cano = train.groupby(['bacno','cano'])['time'].mean().rename(\"mean_time_cano\").reset_index()\n",
    "train = train.merge(mean_time_cano)\n",
    "mean_time_cano = test.groupby(['bacno','cano'])['time'].mean().rename(\"mean_time_cano\").reset_index()\n",
    "test = test.merge(mean_time_cano)\n",
    "\n",
    "median_time_cano = train.groupby(['bacno','cano'])['time'].median().rename(\"median_time_cano\").reset_index()\n",
    "train = train.merge(median_time_cano)\n",
    "median_time_cano = test.groupby(['bacno','cano'])['time'].median().rename(\"median_time_cano\").reset_index()\n",
    "test = test.merge(median_time_cano)\n",
    "\n",
    "train = train.sort_values(by = ['total_seconds'])\n",
    "train['total_seconds_diff'] = train.groupby(['bacno'])['total_seconds'].diff()\n",
    "train['total_seconds_diff'] = train['total_seconds_diff'].fillna(0)\n",
    "train = train.sort_values(by = ['total_seconds'])\n",
    "train['total_seconds_diff_cano'] = train.groupby(['bacno','cano'])['total_seconds'].diff()\n",
    "train['total_seconds_diff_cano'] = train['total_seconds_diff_cano'].fillna(0)\n",
    "train['total_seconds_diff_by_freq'] = train['total_seconds_diff']/train['frequency']\n",
    "train['total_seconds_diff_by_freq'] = train['total_seconds_diff_by_freq'].fillna(train.total_seconds_diff_by_freq.mean())\n",
    "\n",
    "test = test.sort_values(by = ['total_seconds'])\n",
    "test['total_seconds_diff'] = test.groupby(['bacno'])['total_seconds'].diff()\n",
    "test['total_seconds_diff'] = test['total_seconds_diff'].fillna(0)\n",
    "test = test.sort_values(by = ['total_seconds'])\n",
    "test['total_seconds_diff_cano'] = test.groupby(['bacno','cano'])['total_seconds'].diff()\n",
    "test['total_seconds_diff_cano'] = test['total_seconds_diff_cano'].fillna(0)\n",
    "test['total_seconds_diff_by_freq'] = test['total_seconds_diff']/test['frequency']\n",
    "test['total_seconds_diff_by_freq'] = test['total_seconds_diff_by_freq'].fillna(test.total_seconds_diff_by_freq.mean())\n",
    "\n",
    "train = train.sort_values(by = ['total_seconds'])\n",
    "train['conam_diff_cano'] = train.groupby(['bacno','cano'])['conam'].diff()\n",
    "test = test.sort_values(by = ['total_seconds'])\n",
    "test['conam_diff_cano'] = test.groupby(['bacno','cano'])['conam'].diff()\n",
    "\n",
    "train = train.sort_values(by = ['total_seconds'])\n",
    "train['conam_diff_cano_abs'] = train.groupby(['bacno','cano'])['conam'].diff().abs(\n",
    "test = test.sort_values(by = ['total_seconds'])\n",
    "test['conam_diff_cano_abs'] = test.groupby(['bacno','cano'])['conam'].diff().abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fillna for Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:51:05.044758Z",
     "start_time": "2019-11-11T08:51:04.546072Z"
    }
   },
   "outputs": [],
   "source": [
    "data[['flbmk', 'flg_3dsmk']] = data[['flbmk','flg_3dsmk']].fillna(-999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:34:46.185875Z",
     "start_time": "2019-11-29T13:34:46.177891Z"
    }
   },
   "outputs": [],
   "source": [
    "# RFECV\n",
    "# from sklearn.feature_selection import RFECV\n",
    "# clf = lgbm.LGBMClassifier() \n",
    "# rfecv = RFECV(estimator=clf, step=1, cv=5,scoring='f1')   #5-fold cross-validation\n",
    "# rfecv = rfecv.fit(X_train, y_train)\n",
    "\n",
    "# print('Optimal number of features :', rfecv.n_features_)\n",
    "# print('Best features :', x_train.columns[rfecv.support_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T09:51:36.789064Z",
     "start_time": "2019-11-11T09:51:36.141290Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = train[['fraud_ind']]\n",
    "x_train = train[['acqic', 'bacno', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "                 'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm','week',\n",
    "                 'stocn_csmcu','frequency_cano','cano_sequence','total_seconds_diff',\n",
    "                 'bacno_stocn','cano_stocn','transation_count_cano',\n",
    "                 'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd','loctm',\n",
    "                 'ecfg_scity','amtby_median_amount_cano','acqic_duplicated_count',   \n",
    "                 'flg_3dsmk_cat_flbmk_cat','conam_diff_cano']]\n",
    "testx = test[['acqic', 'bacno', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "                 'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm','week',\n",
    "                 'stocn_csmcu','frequency_cano','cano_sequence','total_seconds_diff',\n",
    "                 'bacno_stocn','cano_stocn','transation_count_cano',\n",
    "                 'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd','loctm',\n",
    "                 'ecfg_scity','amtby_median_amount_cano','acqic_duplicated_count',   \n",
    "                 'flg_3dsmk_cat_flbmk_cat','conam_diff_cano']]\n",
    "\n",
    "#train_test_split\n",
    "# X_train,X_valid,y_train,y_valid = model_selection.train_test_split(x_train, y_train, random_state=12, test_size=0.2) \n",
    "\n",
    "##All data\n",
    "X_train = x_train\n",
    "y_train = y_train\n",
    "\n",
    "##day1-day60 as train set,day60-day90 as validation set\n",
    "# X_valid = x_train[x_train.locdt>60]\n",
    "# X_train = x_train[x_train.locdt<61]\n",
    "\n",
    "##Time series split\n",
    "# y_train = train[['fraud_ind','locdt','time']]\n",
    "# x_train = train[['locdt','time']]\n",
    "# testx = test[['locdt','time',]]\n",
    "# X = x_train.sort_values('locdt')\n",
    "# X = X.sort_values('time').drop(['locdt','time'], axis=1)\n",
    "# y = y_train.sort_values('locdt')\n",
    "# y = y.sort_values('time').drop(['locdt','time'], axis=1)\n",
    "# #X_test = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)\n",
    "# X_test = testx.drop(['locdt','time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversample for imbalance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:25:40.476901Z",
     "start_time": "2019-11-03T09:25:15.129Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "\n",
    "# # # concatenate our training data back together\n",
    "# X = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# # separate minority and majority classes\n",
    "# not_fraud = X[X.fraud_ind==0]\n",
    "# fraud = X[X.fraud_ind==1]\n",
    "\n",
    "# # upsample minority\n",
    "# fraud_upsampled = resample(fraud,\n",
    "#                           replace=True, # sample with replacement\n",
    "#                           n_samples=len(not_fraud), # match number in majority class\n",
    "#                           random_state=27) # reproducible results\n",
    "\n",
    "# # combine majority and upsampled minority\n",
    "# upsampled = pd.concat([not_fraud, fraud_upsampled])\n",
    "# y_train = upsampled.fraud_ind\n",
    "# X_train = upsampled.drop('fraud_ind', axis=1)\n",
    "\n",
    "# # check new class counts\n",
    "# upsampled.fraud_ind.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# specifying catfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:25:40.477898Z",
     "start_time": "2019-11-03T09:25:15.521Z"
    }
   },
   "outputs": [],
   "source": [
    "x_cat_feature = train[['acqic', 'bacno', 'contp', 'csmcu', 'ecfg', 'etymd','flbmk', \n",
    "                       'flg_3dsmk', 'hcefg', 'insfg', 'iterm','mcc', 'mchno', 'ovrlt', 'stscd',\n",
    "                       'stocn_csmcu' , 'cano_sequence' ,'ecfg_scity']]\n",
    "\n",
    "cat_features_names = x_cat_feature.columns # specifying names of categorical features\n",
    "cat_features = [x_train.columns.get_loc(col) for col in cat_features_names]\n",
    "print(cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:24:40.826954Z",
     "start_time": "2019-11-03T09:24:28.001Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "#parameters for Catboost\n",
    "    \n",
    "#      'l2_leaf_reg': [0,25,50]\n",
    "}\n",
    "\n",
    "#parameters for lgbm\n",
    "#     'max_depth': [34,35,36],\n",
    "#     'num_leaves': [250,500,1000],\n",
    "#     'min_child_samples': [18,19,20,21,22],\n",
    "#     'min_child_weight': [0.001,0.002]\n",
    "#     'feature_fraction': [0.6, 0.8, 1],\n",
    "#     'bagging_fraction': [0.8,0.9,1],\n",
    "#     'bagging_freq': [2,3,4],\n",
    "#     'reg_alpha': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5],    \n",
    "#     'reg_lambda': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5],\n",
    "#     'cat_smooth': [0,10,20]\n",
    "\n",
    "CB = CatBoostClassifier(task_type = \"GPU\",random_seed=1)\n",
    "\n",
    "# fit_params={\"early_stopping_rounds\":50, \n",
    "#             \"eval_metric\" : \"auc\", \n",
    "#             \"eval_set\" : [[X_valid , y_valid]]}\n",
    "\n",
    "gsearch = GridSearchCV(CB, param_grid=parameters, scoring='f1', cv=3)\n",
    "gsearch.fit(X_train,y_train)#**fit_params\n",
    "print('最佳參數:{0}'.format(gsearch.best_params_))\n",
    "print('最佳分數:{0}'.format(gsearch.best_score_))\n",
    "print(gsearch.cv_results_['mean_test_score'])\n",
    "print(gsearch.cv_results_['params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest, Lightgbm and Catboost modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier( criterion = 'entropy',\n",
    "                                 n_estimators=1100,\n",
    "                                 min_samples_split=140,\n",
    "                                 min_samples_leaf=1,\n",
    "                                 n_jobs=-1) \n",
    "\n",
    "clf_lgbm = lgbm.LGBMClassifier(objective = 'binary', \n",
    "                               is_unbalance = True, \n",
    "                               metric = 'binary_logloss,auc', \n",
    "                               max_depth = 35, \n",
    "                               num_leaves = 1000, \n",
    "                               learning_rate = 0.01, \n",
    "                               feature_fraction = 0.6, \n",
    "                               min_child_samples=21, \n",
    "                               min_child_weight=0.001, \n",
    "                               bagging_fraction = 1, \n",
    "                               bagging_freq = 2, \n",
    "                               reg_alpha = 0.001, \n",
    "                               reg_lambda = 0.3, \n",
    "                               cat_smooth = 0, \n",
    "                               num_iterations = 7000)\n",
    "\n",
    "clf_cb =CatBoostClassifier(random_seed=1,task_type = \"GPU\",iterations =2000,l2_leaf_reg = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:24:40.828947Z",
     "start_time": "2019-11-03T09:24:35.550Z"
    }
   },
   "outputs": [],
   "source": [
    "##F1 Score\n",
    "# def f1_eval(y_pred, dtrain):\n",
    "#     y_true = dtrain.get_label()\n",
    "#     err = 1-f1_score(y_true, np.round(y_pred))\n",
    "#     return 'f1_err', err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:24:40.829945Z",
     "start_time": "2019-11-03T09:24:37.916Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_set  = [(X_valid , y_valid)]\n",
    "\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "clf_lgbm.fit(X_train, y_train ,eval_set=eval_set,early_stopping_rounds=50,eval_metric='f1_eval')\n",
    "clf_lgbm.fit(X_train, y_train)\n",
    "\n",
    "clf_cb.fit(X_train, y_train,cat_features = cat_features)\n",
    "clf_cb.fit(X_train, y_train,cat_features = cat_features,plot=True,eval_set=(X_valid , y_valid),use_best_model=True,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T09:52:42.149358Z",
     "start_time": "2019-10-20T09:28:22.094Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf_lgbm.predict(X_valid)\n",
    "f1_scores[clf_name] = f1_score(y_pred, y_valid)\n",
    "print(metrics.classification_report(y_pred, y_valid))\n",
    "print(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tss = TimeSeriesSplit(n_splits=5)\n",
    "# # kfold = KFold(n_splits=5)\n",
    "# scores = cross_val_score(clf, X, y, cv=tss, scoring = 'f1',n_jobs =-1) \n",
    "# print(scores)\n",
    "# print (\"mean validation F1:\",\n",
    "#        \"%0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T12:09:36.222214Z",
     "start_time": "2019-10-20T12:09:36.185313Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat((pd.DataFrame(x_train.columns, columns = ['variable']), \n",
    "           pd.DataFrame(clf.feature_importances_, columns = ['importance'])), \n",
    "          axis = 1).sort_values(by='importance', ascending = False)[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T12:10:33.522341Z",
     "start_time": "2019-10-20T12:09:50.435345Z"
    }
   },
   "outputs": [],
   "source": [
    "test['fraud_ind'] =  clf.predict(testx)\n",
    "submit = submit[['txkey']]\n",
    "submit=pd.merge(submit,test,on='txkey') \n",
    "submit['fraud_ind'] = submit['fraud_ind'].astype(int)\n",
    "submit = submit[['txkey','fraud_ind']]\n",
    "submit.to_csv('submision.csv', index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
